
<!DOCTYPE html>

<html lang="zh-CN">
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>PyTorch模型转ONNX</title>
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
<style>.container {
    max-width: 800px;
    margin: 0 auto;
    padding: 24px 40px;
    background-color: #fff;
    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    border-radius: 8px;
    }
.chart-container {
    position: relative;
    margin: 3em auto;
    max-width: 500px;
    height: 400px;
    overflow: visible;
    aspect-ratio: 7/5;}
img {
    display: block;
    overflow: hidden;
    max-width: 100%;
    max-height: 280px;
    margin: 1em auto;
    border-radius: 8px;
    }
h5 {
    font-size: 16px;
    }
body {
    font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif;
    line-height: 1.8;
    color: #333;
    margin: 0 24px 0 24px;
    padding: 0;
    background-color: #f9f9f9;
    font-size: 16px;
    max-width: None;
    }
h1, h2, h3, h4 {
    color: #1a1a1a;
    font-weight: 600;
    line-height: 1.3;
    font-size: 28px;
    margin-top: 24px;
    margin-bottom: 20px;
    }
h1 {
    font-size: 28px;
    margin-bottom: 20px;
    border-bottom: 2px solid #eee;
    padding-bottom: 0.4em;
    margin-top: 24px;
    }
h2 {
    font-size: 22px;
    margin-top: 1.5em;
    margin-bottom: 0.6em;
    border-bottom: 1px solid #eee;
    padding-bottom: 0.3em;
    }
h3 {
    font-size: 20px;
    margin-top: 1.2em;
    margin-bottom: 0.5em;
    }
h4 {
    font-size: 18px;
    margin-top: 1em;
    margin-bottom: 0.4em;
    }
p {
    margin-bottom: 1.2em;
    font-size: 16px;
    /* Medium-like font size */
    }
ul, ol {
    margin-bottom: 1.2em;
    padding-left: 1.5em;
    }
li {
    margin-bottom: 0.5em;
    }
code {
    font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace;
    background-color: #f0f0f0;
    padding: 0.2em 0.4em;
    border-radius: 3px;
    font-size: 0.9em;
    }
pre {
    background-color: #f5f5f5;
    padding: 1em;
    border-radius: 4px;
    overflow-x: auto;
    font-size: 0.9em;
    }
pre code {
    background-color: transparent;
    padding: 0;
    }
a {
    color: #007bff;
    text-decoration: none;
    }
a:hover {
    text-decoration: underline;
    }
blockquote {
    border-left: 4px solid #007bff;
    padding-left: 1em;
    margin-left: 0;
    font-style: italic;
    color: #555;
    }
table {
    width: 100%;
    border-collapse: collapse;
    margin-bottom: 1.2em;
    }
th, td {
    border: 1px solid #ddd;
    padding: 0.8em;
    text-align: left;
    }
th {
    background-color: #f2f2f2;
    }
.current-time {
    text-align: right;
    font-size: 0.9em;
    color: #777;
    margin-bottom: 1em;
    }
.key-points {
    background-color: #e7f3ff;
    border-left: 5px solid #007bff;
    padding: 15px;
    margin: 20px 0;
    border-radius: 4px;
    }
.key-points h4 {
    margin-top: 0;
    color: #0056b3;
    }
        .chart-container canvas {
            width: 100% !important;
            height: 100% !important;
            object-fit: contain;
}

@media only screen and (max-device-width: 768px) {
            body {
                padding: 0;
                margin: 0;
                font-family: PingFang SC;
                font-size: 15px;
                line-height: 1.5;
            }

            .container {
                padding: 0;
                margin: 16px 20px 30px;
                box-shadow: none;
            }

            h1,
            h2,
            h3,
            h4 {
                font-family: PingFang SC;
            }

            h1 {
                font-size: 1.87em;
                line-height: 1.6;
                margin-bottom: 0.5em;
                text-align: center;
            }

            h2 {
                font-size: 1.6em;
                font-weight: 600;
                margin-top: 1.3em;
                margin-bottom: 0.8em;
                border-bottom: 1px solid #eee;
                padding-bottom: 0.5em;
            }

            h3 {
                font-size: 1.2em;
                font-weight: 600;
                margin-top: 1em;
                margin-bottom: 0.6em;
            }

            h4 {
                font-size: 1.1em;
                font-weight: 500;
                margin-top: 1em;
                margin-bottom: 0.5em;
                font-style: normal;
            }

            h5 {
                font-size: 1em;
                font-weight: 500;
                margin-bottom: 1.2em;
            }

            ul,
            ol {
                font-size: 1em; /* Equivalent to 17.6px if base is 16px */
                font-weight: 400;
                margin-bottom: 1.2em;
                line-height: 1.8;
            }

            p {
                font-size: 1em;
                line-height: 1.8; /* Equivalent to 17.6px if base is 16px */
                font-weight: 400;
                margin-top: 0.8em;
                margin-bottom: 0.8em;
            }

            blockquote {
                padding: 1em 1.2em;

            p {
                margin: 0;
            }
        }

        figcaption {
            margin-top: 0.5em;
            font-size: 0.8em; /* Equivalent to 17.6px if base is 16px */
            font-weight: 400;
            text-align: center;
            font-style: normal;
            color: #7F8896;
        }

        img {
            display: block;
            overflow: hidden;
            max-width: 100%;
            max-height: 335px;
            margin: 1em auto;
            border-radius: 8px;
        }
        }</style>
</head>
<body>

<div class="container">
<h1>PyTorch模型转ONNX指南</h1>
<p><strong>作者：杨志</strong></p>

<section id="problem-background">
<h2>问题背景与核心要素</h2>
<p>在当前的机器学习领域，模型的研发与部署是两个紧密相连却又各具挑战的环节。PyTorch 以其灵活性、易用性和强大的社区支持，成为学术研究和模型训练的首选框架之一。然而，当模型从研究阶段走向实际应用部署时，开发者往往面临诸多挑战。</p>
<h3>引言：为什么需要模型转换？</h3>
<p><strong>PyTorch</strong>：作为一款以 Python 优先，支持动态计算图的深度学习框架，PyTorch 极大地简化了复杂模型的构建和快速迭代过程。其完善的生态系统和丰富的预训练模型库，使其在科研领域占据主导地位。 <a href="https://pytorch.org/" target="_blank">PyTorch官方网站</a></p>
<p><strong>模型部署的挑战</strong>：训练完成的 PyTorch 模型（通常为 <code>.pth</code> 或 <code>.pt</code> 文件）在部署时会遇到以下问题：</p>
<ul>
<li><strong>环境依赖</strong>：目标部署环境可能不适合安装完整的 PyTorch 库及其依赖，尤其是在资源受限的边缘设备或对启动速度有要求的服务端。</li>
<li><strong>跨平台/框架兼容性</strong>：不同的硬件平台（CPU, GPU, NPU, TPU等）和推理框架（TensorRT, OpenVINO, CoreML, TensorFlow Lite等）对模型格式有不同要求。</li>
<li><strong>性能优化</strong>：原始 PyTorch 模型可能未针对特定硬件进行极致优化，需要借助专门的推理引擎和优化工具来提升速度和降低资源消耗。</li>
</ul>
<p><strong>ONNX (Open Neural Network Exchange) 的诞生</strong>：为了解决上述模型互操作性问题，ONNX 应运而生。它是一种为机器学习模型设计的开放标准格式。 <a href="https://onnx.ai/" target="_blank">ONNX官方网站</a></p>
<blockquote>
                ONNX 的核心价值在于实现"一次训练，多处部署"。它充当了不同深度学习框架和硬件加速器之间的桥梁，允许开发者将一个框架中训练的模型导出为 ONNX 格式，然后在另一个支持 ONNX 的框架或硬件上进行推理。
                <a href="https://docs.pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html" target="_blank">PyTorch ONNX导出教程</a>
</blockquote>
</section>
<section id="core-problem">
<h3>核心问题提炼：</h3>
<ul>
<li>如何将 PyTorch 模型，特别是像 OpenAI Whisper 这样包含 Transformer 结构的复杂模型，高效且正确地转换为 ONNX 格式？</li>
<li>在这一转换过程中，<code>sherpa-onnx/scripts/whisper/export-onnx.py</code> 脚本扮演了怎样的角色？其具体的实现方法和关键考量是什么？</li>
</ul>
</section>
<section id="objective-structure">
<h3>本文目标与结构概述：</h3>
<p>本文旨在以 <code>sherpa-onnx</code> 项目中用于导出 Whisper 模型的 <code>export-onnx.py</code> 脚本为例，深入剖析从 PyTorch 模型到 ONNX 格式的转换流程。我们将详细探讨其中的关键技术点、参数选择、常见问题及其解决方案。通过阅读本文，读者不仅能理解通用的模型转换原理，还能掌握针对特定复杂模型（如语音识别领域的 Transformer 模型）的导出技巧和最佳实践。</p>
<p>文章结构将围绕以下几个核心部分展开：ONNX 与 PyTorch <code>torch.onnx.export</code> 函数概览、<code>sherpa-onnx</code> Whisper 导出脚本的深度解读、PyTorch 模型转 ONNX 的进阶技巧与常见问题、导出后验证与使用，最后进行总结并提供最佳实践建议。</p>
</section>
<section id="onnx-pytorch-export-overview">
<h2>ONNX与PyTorch <code>torch.onnx.export</code> 概览</h2>
<h3>ONNX 基础</h3>
<p><strong>定义</strong>：ONNX (Open Neural Network Exchange) 是一种用于表示机器学习模型的开放格式。它旨在促进不同人工智能框架之间的互操作性。 <a href="https://onnx.ai/onnx/intro/" target="_blank">ONNX简介</a></p>
<p><strong>组成</strong>：一个 ONNX 模型主要由以下部分构成：</p>
<ul>
<li><strong>计算图 (Graph)</strong>：定义了模型的计算流程，由一系列节点（算子）和它们之间的数据流（张量）组成。</li>
<li><strong>节点 (Node)</strong>：代表一个操作，如卷积 (Conv)、矩阵乘法 (MatMul)、激活函数 (ReLU) 等。每个节点有输入、输出和属性。</li>
<li><strong>张量 (Tensor)</strong>：多维数组，是模型中数据的主要载体。</li>
<li><strong>初始值 (Initializer)</strong>：存储模型参数（如权重、偏置）的张量，它们是图的输入之一，但在运行时通常被视为常量。</li>
</ul>
<p><strong>优势</strong>：</p>
<ul>
<li><strong>框架无关性</strong>：允许在不同框架（如 PyTorch, TensorFlow, Keras, MXNet）之间迁移模型。</li>
<li><strong>硬件加速支持</strong>：众多硬件供应商（如 NVIDIA, Intel, Qualcomm, ARM）为其芯片提供基于 ONNX 的优化和加速。</li>
<li><strong>模型可视化与优化</strong>：可以使用如 Netron 这样的工具可视化 ONNX 模型结构，也可以应用图优化技术（如常量折叠、算子融合）来提升性能。</li>
</ul>
<p><strong>ONNX Runtime</strong>：由微软开发并开源的高性能推理引擎，专门用于执行 ONNX 模型。它支持多种硬件加速器和操作系统，能够显著提升模型在CPU和GPU上的推理速度。 <a href="https://onnxruntime.ai/" target="_blank">ONNX Runtime官方网站</a></p>
<h3>PyTorch <code>torch.onnx.export</code> 函数详解</h3>
<p>PyTorch 通过 <code>torch.onnx</code> 模块提供将模型导出为 ONNX 格式的功能，其核心函数是 <code>torch.onnx.export()</code>。</p>
<p><strong>核心功能</strong>：该函数通过执行一次模型（使用提供的示例输入），记录下计算过程中涉及的 PyTorch 算子操作轨迹，然后将这些轨迹转换为符合 ONNX规范的计算图。 <a href="https://pytorch.org/docs/stable/onnx.html" target="_blank">torch.onnx官方文档</a></p>
<h4>关键参数解析：</h4>
<ul>
<li><code>model</code> (<code>torch.nn.Module</code>): 需要导出的 PyTorch 模型实例。</li>
<li><code>args</code> (tuple): 模型的示例输入，也称为 "dummy input"。其形状和类型应与模型实际期望的输入一致。对于有多个输入的模型，<code>args</code> 应为一个元组，包含所有输入。</li>
<li><code>f</code> (str or file-like object): 导出的 ONNX 模型将保存到的文件路径或一个可写的类文件对象。</li>
<li><code>export_params</code> (bool, default=True): 是否将模型中训练好的参数（权重和偏置）一并导出。通常应保持为 <code>True</code>。</li>
<li><code>opset_version</code> (int): 指定要使用的 ONNX 算子集版本。不同的版本支持不同的算子集和行为。选择合适的 <code>opset_version</code> 对于确保模型兼容性和算子支持至关重要。例如，<code>opset_version=11</code> 或 <code>opset_version=17</code> 是常用的选择。</li>
<li><code>do_constant_folding</code> (bool, default=True): 是否执行常量折叠优化。常量折叠会在导出过程中预计算那些输入为常量的节点，从而简化图结构并可能提升性能。建议在部署时开启。</li>
<li><code>input_names</code> (list of str, optional): 为 ONNX 模型的输入节点指定名称。例如 <code>['input_ids', 'attention_mask']</code>。这有助于后续使用 ONNX Runtime 推理时按名称提供输入。</li>
<li><code>output_names</code> (list of str, optional): 为 ONNX 模型的输出节点指定名称。例如 <code>['logits']</code>。</li>
<li><code>dynamic_axes</code> (dict, optional): 指定输入/输出张量中哪些维度是动态的（可变的）。这对于处理如可变序列长度（NLP, ASR）、可变批次大小等场景至关重要。
                    <br/>例如：<code>{'input_ids': {0: 'batch_size', 1: 'sequence_length'}, 'output_logits': {0: 'batch_size'}}</code> 表示 <code>input_ids</code> 的第0维是动态的（名为 'batch_size'），第1维也是动态的（名为 'sequence_length'），而 <code>output_logits</code> 的第0维是动态的。</li>
<li><code>verbose</code> (bool, default=False): 是否在导出过程中打印详细的日志信息，有助于调试。</li>
</ul>
<h4>导出器版本 (Exporter Versions)：</h4>
<p>PyTorch 的 ONNX 导出器经历了发展，主要有两个版本：</p>
<ul>
<li><strong>TorchScript-based exporter</strong>: 这是早期版本和 PyTorch 1.x 中的默认行为。它依赖于 TorchScript（通过 <code>torch.jit.trace()</code> 或 <code>torch.jit.script()</code>）来捕获模型图。
                    <ul>
<li><code>torch.jit.trace()</code>：通过执行一次模型来记录操作。对于包含数据依赖控制流（如依赖张量值的 <code>if</code> 语句）的模型，trace 模式可能无法正确捕捉所有路径。</li>
<li><code>torch.jit.script()</code>：尝试直接解析 Python 代码来理解模型结构，对控制流有更好的支持，但要求代码符合 TorchScript 的语法子集。</li>
<li>局限性：对 Python 的一些动态特性支持有限，处理复杂控制流和某些高级 Python 结构时可能遇到困难。 <a href="https://pytorch.org/docs/stable/onnx_torchscript.html" target="_blank">TorchScript-based ONNX Exporter</a></li>
</ul>
</li>
<li><strong>TorchDynamo-based exporter</strong>: 从 PyTorch 2.0 开始引入，并逐渐成为推荐的导出方式。可以通过 <code>torch.onnx.export(..., dynamo=True)</code> (PyTorch 2.5+) 或早期的 <code>torch.onnx.dynamo_export()</code> (已不推荐) 来使用。
                    <ul>
<li>它利用 TorchDynamo 技术，通过分析 Python字节码来捕获 FX Graph (一种更灵活的图表示)。</li>
<li>优势：对 Python 的动态特性和原生控制流有更好的支持，通常能更准确地导出复杂模型，并且在导出过程中的内存使用也可能更优。 <a href="https://pytorch.org/docs/stable/onnx_dynamo.html" target="_blank">TorchDynamo-based ONNX Exporter</a></li>
</ul>
</li>
</ul>
<div class="chart-container">
<canvas id="exporterComparisonChart"></canvas>
</div>
<script>
                const exporterCtx = document.getElementById('exporterComparisonChart').getContext('2d');
                new Chart(exporterCtx, {
                    type: 'bar',
                    data: {
                        labels: ['动态控制流支持', 'Python特性兼容性', '复杂模型处理', '导出稳定性 (未来趋势)'],
                        datasets: [{
                            label: 'TorchScript-based Exporter',
                            data: [2, 2.5, 3, 2.5], // Illustrative scores 1-5
                            backgroundColor: 'rgba(255, 99, 132, 0.7)',
                            borderColor: 'rgba(255, 99, 132, 1)',
                            borderWidth: 1
                        }, {
                            label: 'TorchDynamo-based Exporter',
                            data: [4, 4.5, 4, 4.5], // Illustrative scores 1-5
                            backgroundColor: 'rgba(54, 162, 235, 0.7)',
                            borderColor: 'rgba(54, 162, 235, 1)',
                            borderWidth: 1
                        }]
                    },
                    options: {
                        responsive: true,
                        maintainAspectRatio: false,
                        scales: {
                            y: {
                                beginAtZero: true,
                                max: 5,
                                title: {
                                    display: true,
                                    text: '支持程度/能力 (示意评分1-5)'
                                }
                            }
                        },
                        plugins: {
                            title: {
                                display: true,
                                text: 'PyTorch ONNX导出器特性对比 (示意)',
                                font: { size: 16 }
                            },
                            legend: {
                                position: 'top',
                            }
                        }
                    }
                });
            </script>
<h4>导出前置准备：</h4>
<p>一个至关重要的步骤是在调用 <code>torch.onnx.export()</code> 之前，将模型设置为评估模式：</p>
<pre><code>model.eval()</code></pre>
<p>或者等效的 <code>model.train(False)</code>。这是因为像 <code>Dropout</code> 层和 <code>BatchNorm</code> 层这样的模块在训练模式和评估模式下的行为是不同的。在导出用于推理的模型时，必须确保它们处于评估（推理）状态，否则可能导致导出的 ONNX 模型行为不正确或性能不佳。 <a href="https://learn.microsoft.com/en-us/windows/ai/windows-ml/tutorials/pytorch-convert-model" target="_blank">微软PyTorch转ONNX教程强调model.eval()</a></p>
</section>

<section id="sherpa-onnx-whisper-export">
<h2>案例分析：<code>export-onnx.py</code> 脚本解读</h2>
<p>以一个将 Whisper 模型从 PyTorch 实现转换为 ONNX 格式的脚本 <code>export-onnx.py</code> 为例，我们可以深入了解复杂 Transformer 模型导出的具体实现细节。这个脚本来自开源项目，是理解复杂模型导出流程的典型案例。</p>


<h3>案例中使用的模型简介</h3>
<p>本案例中的脚本主要处理 Whisper 模型，以下是相关背景信息：</p>
<p><strong>Whisper 模型</strong>: 由 OpenAI 开发的基于 Transformer 架构的多语言语音识别模型。该模型在大量弱监督数据上训练，具有较高的识别准确率和跨语言性能。 <a href="https://openai.com/index/whisper/" target="_blank">OpenAI Whisper介绍</a></p>

<ul>
<li><strong>架构特点</strong>: Whisper 采用标准的 Encoder-Decoder Transformer 结构。
                    <ul>
<li><strong>Encoder</strong>: 接收音频的梅尔频谱图 (Log Mel Spectrogram) 作为输入，通过多层 Transformer Encoder Block 将其编码为一系列声学特征表示。</li>
<li><strong>Decoder</strong>: 这是一个自回归 (auto-regressive) 的 Transformer Decoder。它接收 Encoder 的输出以及先前已生成的 Token 序列作为输入，预测下一个 Token 的概率分布。</li>
</ul>
</li>
<li><strong>输入</strong>: 通常是处理成固定长度（如30秒）音频片段的对数梅尔频谱图。</li>
<li><strong>输出</strong>: 对应于输入音频转录文本的 Token ID 序列。</li>
</ul>
<p>Esperanto Technologies 在一篇博客中详细讨论了 Whisper 模型的架构及其到 ONNX 的转换，指出其遵循典型的 Transformer 结构。 <a href="https://www.esperanto.ai/blog/adapting-whisper-models-to-et-soc-1-architecture-and-exporting-them-to-onnx/" target="_blank">Adapting Whisper Models to ET-SoC-1 Architecture and Exporting Them to ONNX</a></p>

<h3><code>export-onnx.py</code> 脚本目标与设计</h3>
<p>分析的 <code>export-onnx.py</code> 脚本的核心目标是将 Whisper 模型的 PyTorch 版本转换为能被 ONNX 运行时加载和使用的模型文件。</p>

<p><strong>核心设计理念</strong>：</p>
<ul>
<li><strong>组件化导出</strong>: Whisper 模型的 Encoder 和 Decoder 功能和调用模式不同（Encoder 对每段音频执行一次，Decoder 则在自回归生成过程中对每个 Token 执行一次）。因此，脚本将它们分别导出为两个独立的 ONNX 模型：<code><model_name>-encoder.onnx</model_name></code> 和 <code><model_name>-decoder.onnx</model_name></code>。这种分离设计更符合实际推理流程，并有助于优化。</li>
<li><strong>Tokenizer 配置生成</strong>: 脚本还会生成一个 <code><model_name>-tokens.txt</model_name></code> 文件，其中包含了 Whisper 模型使用的 Tokenizer 的词汇表。这个文件对于将模型输出的 Token ID 转换回可读文本至关重要。</li>
<li><strong>多尺寸模型支持</strong>: 脚本支持导出 Whisper 模型的不同尺寸版本，如 <code>tiny.en</code>, <code>base</code>, <code>small</code>, <code>medium</code>, <code>large</code>, <code>large-v1</code>, <code>large-v2</code>, <code>large-v3</code> 等。这些预训练模型可以直接从 Hugging Face Hub 下载。 <a href="https://k2-fsa.github.io/sherpa/onnx/pretrained_models/whisper/export-onnx.html" target="_blank">sherpa-onnx Whisper模型导出文档</a></li>
<li><strong>INT8 量化支持</strong>: 为了进一步优化模型在部署时的性能（速度和体积），脚本通常支持（或配合其他工具）将导出的 FP32 ONNX 模型量化为 INT8 格式，生成如 <code><model_name>-encoder.int8.onnx</model_name></code> 的文件。</li>
</ul>
<h3>脚本关键步骤与技术细节分析</h3>
<p>根据提供的完整<code>export-onnx.py</code>源码，我们可以系统分析其核心实现和关键技术点。</p>
<h4>1. 核心类与结构分析</h4>
<p>为了适应ONNX导出，脚本对Whisper的原始实现进行了多项修改和封装。以下是几个关键类的详细分析：</p>
<ol>
<li>
<strong><code>AudioEncoderTensorCache</code>类</strong>
<pre><code>class AudioEncoderTensorCache(nn.Module):
    def __init__(self, inAudioEncoder: AudioEncoder, inTextDecoder: TextDecoder):
        super().__init__()
        self.audioEncoder = inAudioEncoder
        self.textDecoder = inTextDecoder

    def forward(self, x: Tensor):
        audio_features = self.audioEncoder(x)

        n_layer_cross_k_list = []
        n_layer_cross_v_list = []
        for block in self.textDecoder.blocks:
            n_layer_cross_k_list.append(block.cross_attn.key(audio_features))
            n_layer_cross_v_list.append(block.cross_attn.value(audio_features))

        return torch.stack(n_layer_cross_k_list), torch.stack(n_layer_cross_v_list)</code></pre>

该类将Whisper的Encoder组件封装为能够提前生成cross-attention所需key和value的模块。其核心功能是：
<ul>
<li>首先通过<code>audioEncoder</code>处理音频梅尔频谱图输入，得到<code>audio_features</code></li>
<li>对于Decoder中的每个Transformer层，预先计算cross-attention（跨注意力）的key和value投影。</li>
<li>返回两个张量，分别为所有层的cross-attention keys和values的堆叠结果。</li>
</ul>

这种设计使得Encoder的计算结果可以被缓存并重复用于Decoder的每一步推理，而不需要重复计算，优化了推理效率。
</li>
<li>
<strong><code>MultiHeadAttentionCross</code>类</strong>
<pre><code>class MultiHeadAttentionCross(nn.Module):
    def __init__(self, inMultiHeadAttention: MultiHeadAttention):
        super().__init__()
        self.multiHeadAttention = inMultiHeadAttention

    def forward(
        self,
        x: Tensor,
        k: Tensor,
        v: Tensor,
        mask: Optional[Tensor] = None,
    ):
        q = self.multiHeadAttention.query(x)
        wv, qk = self.multiHeadAttention.qkv_attention(q, k, v, mask)
        return self.multiHeadAttention.out(wv)</code></pre>

该类专门处理Decoder中的跨注意力机制，使其接口适应ONNX导出需求：
<ul>
<li>将原始MultiHeadAttention类进行包装，使其可以接受预先计算好的key(k)和value(v)作为输入</li>
<li>首先对输入x计算query投影，然后利用传入的预计算key和value执行注意力计算</li>
<li>最后应用输出投影并返回结果</li>
</ul>

这种设计支持Encoder和Decoder分离导出模式，同时允许复用预计算的key和value。
</li>
<li>
<strong><code>MultiHeadAttentionSelf</code>类</strong>
<pre><code>class MultiHeadAttentionSelf(nn.Module):
    def __init__(self, inMultiHeadAttention: MultiHeadAttention):
        super().__init__()
        self.multiHeadAttention = inMultiHeadAttention

    def forward(
        self,
        x: Tensor,  # (b, n_ctx      , n_state)
        k_cache: Tensor,  # (b, n_ctx_cache, n_state)
        v_cache: Tensor,  # (b, n_ctx_cache, n_state)
        mask: Tensor,
    ):
        q = self.multiHeadAttention.query(x)  # (b, n_ctx, n_state)
        k = self.multiHeadAttention.key(x)  # (b, n_ctx, n_state)
        v = self.multiHeadAttention.value(x)  # (b, n_ctx, n_state)

        k_cache[:, -k.shape[1] :, :] = k  # (b, n_ctx_cache + n_ctx, n_state)
        v_cache[:, -v.shape[1] :, :] = v  # (b, n_ctx_cache + n_ctx, n_state)

        wv, qk = self.multiHeadAttention.qkv_attention(q, k_cache, v_cache, mask)
        return self.multiHeadAttention.out(wv), k_cache, v_cache</code></pre>

该类专门处理Decoder中的自注意力机制，并实现了KV缓存功能：
<ul>
<li>接受当前输入x及其对应的key和value缓存(k_cache和v_cache)作为输入</li>
<li>计算当前输入的query, key和value投影</li>
<li>将新计算的k和v更新到对应缓存中（注意其更新策略是在缓存的尾部进行替换）</li>
<li>利用query和完整的key/value缓存计算注意力结果</li>
<li>返回注意力输出以及更新后的key和value缓存</li>
</ul>

这种KV缓存机制是实现高效自回归解码的关键，避免了重复计算已处理的token对应的key和value投影。
</li>
<li>
<strong><code>ResidualAttentionBlockTensorCache</code>类</strong>
<pre><code>class ResidualAttentionBlockTensorCache(nn.Module):
    def __init__(self, inResidualAttentionBlock: ResidualAttentionBlock):
        super().__init__()
        self.originalBlock = inResidualAttentionBlock
        self.attn = MultiHeadAttentionSelf(inResidualAttentionBlock.attn)
        self.cross_attn = (
            MultiHeadAttentionCross(inResidualAttentionBlock.cross_attn)
            if inResidualAttentionBlock.cross_attn
            else None
        )

    def forward(
        self,
        x: Tensor,
        self_k_cache: Tensor,
        self_v_cache: Tensor,
        cross_k: Tensor,
        cross_v: Tensor,
        mask: Tensor,
    ):
        self_attn_x, self_k_cache_updated, self_v_cache_updated = self.attn(
            self.originalBlock.attn_ln(x), self_k_cache, self_v_cache, mask=mask
        )
        x = x + self_attn_x

        if self.cross_attn:
            x = x + self.cross_attn(
                self.originalBlock.cross_attn_ln(x), cross_k, cross_v
            )

        x = x + self.originalBlock.mlp(self.originalBlock.mlp_ln(x))
        return x, self_k_cache_updated, self_v_cache_updated</code></pre>

该类封装了Transformer Decoder块的完整功能，包括自注意力、跨注意力和前馈网络：
<ul>
<li>同时使用前面定义的<code>MultiHeadAttentionSelf</code>和<code>MultiHeadAttentionCross</code>类来处理自注意力和跨注意力</li>
<li>实现了完整的残差连接和层归一化，与原始Transformer结构一致</li>
<li>将自注意力的KV缓存管理集成到forward过程中</li>
<li>保持了原始前馈网络(MLP)的计算不变</li>
</ul>

通过这种封装，Transformer块可以无缝支持KV缓存和预计算的跨注意力keys/values，同时保留原始计算逻辑。
</li>
<li>
<strong><code>TextDecoderTensorCache</code>类</strong>
<pre><code>class TextDecoderTensorCache(nn.Module):
    def __init__(self, inTextDecoder: TextDecoder, in_n_ctx: int):
        super().__init__()
        self.textDecoder = inTextDecoder
        self.n_ctx = in_n_ctx

        self.blocks = []
        for orginal_block in self.textDecoder.blocks:
            self.blocks.append(ResidualAttentionBlockTensorCache(orginal_block))

    def forward(
        self,
        tokens: Tensor,
        n_layer_self_k_cache: Tensor,
        n_layer_self_v_cache: Tensor,
        n_layer_cross_k: Tensor,
        n_layer_cross_v: Tensor,
        offset: Tensor,
    ):
        x = (
            self.textDecoder.token_embedding(tokens)
            + self.textDecoder.positional_embedding[
                offset[0] : offset[0] + tokens.shape[-1]
            ]
        )
        x = x.to(n_layer_cross_k[0].dtype)

        i = 0
        for block in self.blocks:
            self_k_cache = n_layer_self_k_cache[i, :, : offset[0] + tokens.shape[-1], :]
            self_v_cache = n_layer_self_v_cache[i, :, : offset[0] + tokens.shape[-1], :]
            x, self_k_cache, self_v_cache = block(
                x,
                self_k_cache=self_k_cache,
                self_v_cache=self_v_cache,
                cross_k=n_layer_cross_k[i],
                cross_v=n_layer_cross_v[i],
                mask=self.textDecoder.mask,
            )
            n_layer_self_k_cache[i, :, : offset[0] + tokens.shape[-1], :] = self_k_cache
            n_layer_self_v_cache[i, :, : offset[0] + tokens.shape[-1], :] = self_v_cache
            i += 1

        x = self.textDecoder.ln(x)

        if False:
            # x.shape (1, 3, 384)
            # weight.shape (51684, 384)

            logits = (
                x
                @ torch.transpose(
                    self.textDecoder.token_embedding.weight.to(x.dtype), 0, 1
                )
            ).float()
        else:
            logits = (
                torch.matmul(
                    self.textDecoder.token_embedding.weight.to(x.dtype),
                    x.permute(0, 2, 1),
                )
                .permute(0, 2, 1)
                .float()
            )

        return logits, n_layer_self_k_cache, n_layer_self_v_cache</code></pre>

该类是整个Whisper Decoder的顶层封装，管理多层Transformer块的执行流程：
<ul>
<li>初始化时，将原始Decoder的所有Transformer块封装为<code>ResidualAttentionBlockTensorCache</code>实例</li>
<li>接受tokens输入、自注意力KV缓存、预计算的跨注意力KV以及偏移量(offset)作为参数</li>
<li>首先应用token嵌入和位置嵌入（注意使用offset来选择正确的位置嵌入）</li>
<li>依次通过所有Transformer块处理，同时管理每层的KV缓存</li>
<li>应用最终的层归一化</li>
<li>计算logits输出（通过与token嵌入权重的矩阵乘法）</li>
<li>返回logits和更新后的自注意力KV缓存</li>
</ul>

注意代码中特殊的logits计算方法，修改了矩阵乘法的顺序以提高某些硬件上的性能。
</li>
<li>
<strong><code>modified_audio_encoder_forward</code>函数</strong>
<pre><code>def modified_audio_encoder_forward(self: AudioEncoder, x: torch.Tensor):
    """
    x : torch.Tensor, shape = (batch_size, n_mels, n_ctx)
        the mel spectrogram of the audio
    """
    x = F.gelu(self.conv1(x))
    x = F.gelu(self.conv2(x))
    x = x.permute(0, 2, 1)

    if False:
        # This branch contains the original code
        assert x.shape[1:] == self.positional_embedding.shape, "incorrect audio shape"
        x = (x + self.positional_embedding).to(x.dtype)
    else:
        # This branch contains the actual changes
        assert (
            x.shape[2] == self.positional_embedding.shape[1]
        ), f"incorrect audio shape: {x.shape}, {self.positional_embedding.shape}"
        assert (
            x.shape[1] == self.positional_embedding.shape[0]
        ), f"incorrect audio shape: {x.shape}, {self.positional_embedding.shape}"
        x = (x + self.positional_embedding[: x.shape[1]]).to(x.dtype)

    for block in self.blocks:
        x = block(x)

    x = self.ln_post(x)
    return x</code></pre>

这个函数修改了Whisper AudioEncoder的原始forward方法，主要变化在于处理位置嵌入时支持动态长度：
<ul>
<li>原始代码（if False分支）要求输入形状严格匹配位置嵌入形状</li>
<li>修改后的代码（else分支）允许输入长度小于或等于预设的最大长度，通过切片<code>self.positional_embedding[: x.shape[1]]</code>来支持动态长度</li>
<li>这一修改对于处理不同长度的音频输入至关重要，使模型能接受短于标准30秒的音频</li>
</ul>

脚本通过<code>AudioEncoder.forward = modified_audio_encoder_forward</code>覆盖了原始方法，使这一改动全局生效。
</li>
</ol>
<h4>2. 元数据处理与模型信息保存</h4>
<p><code>add_meta_data</code>函数用于向ONNX模型添加丰富的元数据，这些元数据对于模型的正确加载和使用至关重要：</p>
<pre><code>def add_meta_data(filename: str, meta_data: Dict[str, Any]):
    """Add meta data to an ONNX model. It is changed in-place.

    Args:
      filename:
        Filename of the ONNX model to be changed.
      meta_data:
        Key-value pairs.
    """
    model = onnx.load(filename)

    while len(model.metadata_props):
        model.metadata_props.pop()

    for key, value in meta_data.items():
        meta = model.metadata_props.add()
        meta.key = key
        meta.value = str(value)

    if "large" in filename or "turbo" in filename:
        external_filename = filename.split(".onnx")[0]
        onnx.save(
            model,
            filename,
            save_as_external_data=True,
            all_tensors_to_one_file=True,
            location=external_filename + ".weights",
        )
    else:
        onnx.save(model, filename)
</code></pre>
<p>核心功能包括：</p>
<ul>
<li>清除任何现有元数据并添加新的键值对</li>
<li>对于"large"或"turbo"模型，将权重保存为外部文件(<code>.weights</code>)，解决ONNX文件大小限制问题</li>
<li>在<code>main()</code>函数中可见，添加的元数据十分丰富，包括模型类型、维度信息、tokenizer配置等关键信息</li>
</ul>
<h4>3. Tokenizer处理</h4>
<p>脚本中的<code>convert_tokens</code>函数负责提取和保存Whisper tokenizer的词汇表：</p>
<pre><code>def convert_tokens(name, model):
    whisper_dir = Path(whisper.__file__).parent
    multilingual = model.is_multilingual
    tokenizer = (
        whisper_dir
        / "assets"
        / (multilingual and "multilingual.tiktoken" or "gpt2.tiktoken")
    )
    if not tokenizer.is_file():
        raise ValueError(f"Cannot find {tokenizer}")

    with open(tokenizer, "r") as f:
        contents = f.read()
        tokens = {
            token: int(rank)
            for token, rank in (line.split() for line in contents.splitlines() if line)
        }

    with open(f"{name}-tokens.txt", "w") as f:
        for t, i in tokens.items():
            f.write(f"{t} {i}\n")
</code></pre>
<p>该函数：</p>
<ul>
<li>根据模型是否多语言选择相应的tiktoken文件</li>
<li>解析tiktoken文件并提取token与ID的映射关系</li>
<li>将映射关系写入<code>{name}-tokens.txt</code>文件，供sherpa-onnx运行时使用</li>
</ul>
<h4>4. 主函数流程与模型导出</h4>
<p>在<code>main()</code>函数中，脚本实现了完整的模型加载、导出和量化流程：</p>
<ol>
<li><strong>模型加载与前处理</strong>
<pre><code>args = get_args()
name = args.model
model = whisper.load_model(name)  # 或特殊模型的加载
model.eval()
print(model.dims)
</code></pre>
</li>
<li><strong>Tokenizer处理</strong>
<pre><code>convert_tokens(name=name, model=model)
tokenizer = whisper.tokenizer.get_tokenizer(
    model.is_multilingual, num_languages=model.num_languages
)
</code></pre>
</li>
<li><strong>示例输入准备</strong>
<pre><code>audio = torch.rand(16000 * 2)
audio = whisper.pad_or_trim(audio)
assert audio.shape == (16000 * 30,), audio.shape

if args.model in ("large", "large-v3", "turbo"):
    n_mels = 128
else:
    n_mels = 80
mel = (
    whisper.log_mel_spectrogram(audio, n_mels=n_mels).to(model.device).unsqueeze(0)
)
batch_size = 1
assert mel.shape == (batch_size, n_mels, 30 * 100), mel.shape
</code></pre>
</li>
<li><strong>Encoder导出</strong>
<pre><code>encoder = AudioEncoderTensorCache(model.encoder, model.decoder)
n_layer_cross_k, n_layer_cross_v = encoder(mel)
# ... 验证形状 ...
encoder_filename = f"{name}-encoder.onnx"
torch.onnx.export(
    encoder,
    mel,
    encoder_filename,
    opset_version=opset_version,
    input_names=["mel"],
    output_names=["n_layer_cross_k", "n_layer_cross_v"],
    dynamic_axes={
        "mel": {0: "n_audio", 2: "T"},  # n_audio is also known as batch_size
        "n_layer_cross_k": {1: "n_audio", 2: "T"},
        "n_layer_cross_v": {1: "n_audio", 2: "T"},
    },
)
</code></pre>
</li>
<li><strong>Encoder元数据添加</strong>
<pre><code>encoder_meta_data = {
    "model_type": f"whisper-{name}",
    "version": "1",
    "maintainer": "k2-fsa",
    "n_mels": model.dims.n_mels,
    # ... 各种模型参数与维度信息 ...
    "is_multilingual": int(model.is_multilingual),
    # ... tokenizer相关信息 ...
}
add_meta_data(filename=encoder_filename, meta_data=encoder_meta_data)
</code></pre>
</li>
<li><strong>Decoder导出</strong>
<pre><code>tokens = torch.tensor([[tokenizer.sot, tokenizer.sot, tokenizer.sot]] * n_audio).to(
    mel.device
)  # [n_audio, 3]
decoder = TextDecoderTensorCache(model.decoder, model.dims.n_text_ctx)
n_layer_self_k_cache = torch.zeros(
    (
        len(model.decoder.blocks),
        n_audio,
        model.dims.n_text_ctx,
        model.dims.n_text_state,
    ),
    device=mel.device,
)
n_layer_self_v_cache = torch.zeros(
    # ... 同样的形状 ...
)
offset = torch.zeros(1, dtype=torch.int64).to(mel.device)
logits, n_layer_self_k_cache, n_layer_self_v_cache = decoder(
    tokens,
    n_layer_self_k_cache,
    n_layer_self_v_cache,
    n_layer_cross_k,
    n_layer_cross_v,
    offset,
)

# 第二次推理示例 - 用于验证KV Cache更新
offset = torch.tensor([tokens.shape[1]], dtype=torch.int64).to(mel.device)
tokens = torch.tensor([[tokenizer.sot]] * n_audio).to(mel.device)  # [n_audio, 1]
logits, out_n_layer_self_k_cache, out_n_layer_self_v_cache = decoder(
    tokens,
    n_layer_self_k_cache,
    n_layer_self_v_cache,
    n_layer_cross_k,
    n_layer_cross_v,
    offset,
)

# 实际导出
decoder_filename = f"{name}-decoder.onnx"
torch.onnx.export(
    decoder,
    (
        tokens,
        n_layer_self_k_cache,
        n_layer_self_v_cache,
        n_layer_cross_k,
        n_layer_cross_v,
        offset,
    ),
    decoder_filename,
    opset_version=opset_version,
    input_names=[
        "tokens",
        "in_n_layer_self_k_cache",
        "in_n_layer_self_v_cache",
        "n_layer_cross_k",
        "n_layer_cross_v",
        "offset",
    ],
    output_names=["logits", "out_n_layer_self_k_cache", "out_n_layer_self_v_cache"],
    dynamic_axes={
        "tokens": {0: "n_audio", 1: "n_tokens"},
        "in_n_layer_self_k_cache": {1: "n_audio"},
        "in_n_layer_self_v_cache": {1: "n_audio"},
        "n_layer_cross_k": {1: "n_audio", 2: "T"},
        "n_layer_cross_v": {1: "n_audio", 2: "T"},
    },
)
</code></pre>
</li>
<li><strong>特殊处理large模型</strong>
<pre><code>if "large" in args.model:
    decoder_external_filename = decoder_filename.split(".onnx")[0]
    decoder_model = onnx.load(decoder_filename)
    onnx.save(
        decoder_model,
        decoder_filename,
        save_as_external_data=True,
        all_tensors_to_one_file=True,
        location=decoder_external_filename + ".weights",
    )
</code></pre>
</li>
<li><strong>INT8量化</strong>
<pre><code>print("Generate int8 quantization models")

encoder_filename_int8 = f"{name}-encoder.int8.onnx"
quantize_dynamic(
    model_input=encoder_filename,
    model_output=encoder_filename_int8,
    op_types_to_quantize=["MatMul"],
    weight_type=QuantType.QInt8,
)

decoder_filename_int8 = f"{name}-decoder.int8.onnx"
quantize_dynamic(
    model_input=decoder_filename,
    model_output=decoder_filename_int8,
    op_types_to_quantize=["MatMul"],
    weight_type=QuantType.QInt8,
)
</code></pre>
</li>
</ol>
<h4>5. 动态轴设置的详细分析</h4>
<p>脚本中的动态轴配置是模型导出的关键部分，它定义了哪些维度允许在推理时变化：</p>
<ul>
<li><strong>Encoder的动态轴</strong>:
<pre><code>dynamic_axes={
    "mel": {0: "n_audio", 2: "T"},  # n_audio is also known as batch_size
    "n_layer_cross_k": {1: "n_audio", 2: "T"},
    "n_layer_cross_v": {1: "n_audio", 2: "T"},
},</code></pre>
<ul>
<li><code>mel</code>输入：批次大小(维度0)和时间轴/音频长度(维度2)允许动态变化</li>
<li><code>n_layer_cross_k</code>和<code>n_layer_cross_v</code>输出：这些是预计算的cross-attention keys和values，它们的批次大小(维度1)和时间轴长度(维度2)需要与输入保持匹配</li>
</ul>
</li>
<li><strong>Decoder的动态轴</strong>:
<pre><code>dynamic_axes={
    "tokens": {0: "n_audio", 1: "n_tokens"},
    "in_n_layer_self_k_cache": {1: "n_audio"},
    "in_n_layer_self_v_cache": {1: "n_audio"},
    "n_layer_cross_k": {1: "n_audio", 2: "T"},
    "n_layer_cross_v": {1: "n_audio", 2: "T"},
},</code></pre>
<ul>
<li><code>tokens</code>：允许批次大小(维度0)和token序列长度(维度1)动态变化</li>
<li>KV缓存：只有批次大小(维度1)需要动态，其他维度保持固定</li>
<li>Cross-attention KV：与Encoder输出一致，允许批次和时间轴变化</li>
<li>注意Decoder输出未指定动态轴，这意味着输出将继承输入的动态性</li>
</ul>
</li>
</ul>
<h4>6. INT8量化实现细节</h4>
<p>脚本使用ONNX Runtime的量化API对模型进行后处理量化：</p>
<pre><code>quantize_dynamic(
    model_input=encoder_filename,
    model_output=encoder_filename_int8,
    op_types_to_quantize=["MatMul"],
    weight_type=QuantType.QInt8,
)</code></pre>
<p>关键特点：</p>
<ul>
<li>使用的是动态量化(dynamic quantization)方法，权重被量化为INT8，激活值在运行时动态量化</li>
<li>只对<code>MatMul</code>操作进行量化，这是Transformer模型中计算和内存开销最大的部分</li>
<li>量化类型设置为<code>QInt8</code>，这是一个带符号的8位整数类型，适合权重的分布特性</li>
<li>未使用校准数据集，这简化了量化流程但可能导致精度略低于使用代表性数据进行校准的情况</li>
</ul>
<h4>7. Whisper模型转ONNX的特殊处理</h4>
<p>从代码中可以观察到几个针对Whisper模型的特殊处理：</p>
<ul>
<li><strong>移除30秒限制</strong>: 脚本注释提到移除了Whisper原始30秒音频限制，使其可处理任意长度音频</li>
<li><strong>模型特例处理</strong>: 对于不同的模型版本有不同处理逻辑：
<ul>
<li>large和turbo模型使用128梅尔频带，其他模型使用80梅尔频带</li>
<li>large系列模型的权重保存为外部文件，以处理ONNX protobuf 2GB大小限制</li>
<li>蒸馏模型(distil-medium.en等)需要从特定地址下载模型权重</li>
</ul>
</li>
<li><strong>矩阵乘法计算优化</strong>: 在<code>TextDecoderTensorCache.forward</code>中，通过调整矩阵乘法的顺序提高计算效率：
<pre><code>if False:
    # x.shape (1, 3, 384)
    # weight.shape (51684, 384)
    logits = (
        x
        @ torch.transpose(
            self.textDecoder.token_embedding.weight.to(x.dtype), 0, 1
        )
    ).float()
else:
    logits = (
        torch.matmul(
            self.textDecoder.token_embedding.weight.to(x.dtype),
            x.permute(0, 2, 1),
        )
        .permute(0, 2, 1)
        .float()
    )</code></pre>
</li>
<li><strong>位置编码处理</strong>: 修改了位置编码的应用方式，允许处理动态长度而非严格固定长度</li>
</ul>
<h3><code>export-onnx.py</code> 脚本的命令行参数示例与解读</h3>
<p>根据<code>get_args()</code>函数，脚本接受以下命令行参数：</p>
<pre><code>def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--model",
        type=str,
        required=True,
        # fmt: off
        choices=[
            "tiny", "tiny.en", "base", "base.en",
            "small", "small.en", "medium", "medium.en",
            "large-v1", "large-v2",
            "large", "large-v3", "turbo", # these three have feature dim 128
            "distil-medium.en", "distil-small.en", "distil-large-v2",
            # "distil-large-v3", # distil-large-v3 is not supported!
            # for fine-tuned models from icefall
            "medium-aishell",
            ],
        # fmt: on
    )
    return parser.parse_args()</code></pre>
<p>导出命令示例：</p>
<pre><code class="language-bash"># 导出 tiny.en 模型
python ./scripts/whisper/export-onnx.py --model tiny.en

# 导出 large-v3 模型
python ./scripts/whisper/export-onnx.py --model large-v3

# 导出蒸馏版模型
python ./scripts/whisper/export-onnx.py --model distil-medium.en

# 导出微调后的模型
python ./scripts/whisper/export-onnx.py --model medium-aishell
</code></pre>
<p>脚本支持的模型类型非常丰富，包括:</p>
<ul>
<li>标准Whisper模型（不同大小和语言版本）</li>
<li>新版本模型（large-v1/v2/v3, turbo）</li>
<li>蒸馏模型（distil-medium.en等）</li>
<li>针对特定语言微调的模型（如medium-aishell）</li>
</ul>
<p>特别注意，对于蒸馏模型和微调模型，脚本会检查相应文件是否存在，否则会给出下载指导。</p>
<h3>输出文件解读</h3>
<p>成功执行 <code>export-onnx.py</code> 脚本后，会在输出目录生成以下文件：</p>
<ul>
<li><code>&lt;model_name&gt;-encoder.onnx</code>: Whisper Encoder 的 FP32 ONNX 模型。</li>
<li><code>&lt;model_name&gt;-decoder.onnx</code>: Whisper Decoder 的 FP32 ONNX 模型，包含 KV Cache 处理逻辑。</li>
<li><code>&lt;model_name&gt;-tokens.txt</code>: Tokenizer 的词汇表文件。</li>
<li><code>&lt;model_name&gt;-encoder.int8.onnx</code>: INT8 量化后的 Encoder ONNX 模型。</li>
<li><code>&lt;model_name&gt;-decoder.int8.onnx</code>: INT8 量化后的 Decoder ONNX 模型。</li>
</ul>
<p>对于 <code>large</code> 系列模型，还会生成额外的权重文件：</p>
<ul>
<li><code>&lt;model_name&gt;-encoder.weights</code>: Encoder 的外部权重文件。</li>
<li><code>&lt;model_name&gt;-decoder.weights</code>: Decoder 的外部权重文件。</li>
</ul>
<p>这些输出文件的组织方式与<code>sherpa-onnx</code>的运行时预期完全匹配，可以直接被其识别和加载。</p>
</section>
<section id="advanced-techniques-common-issues">
<h2>PyTorch 模型转 ONNX 的进阶技巧与常见问题</h2>
<p>将 PyTorch 模型转换为 ONNX 格式，尤其对于复杂的 Transformer 模型，往往会遇到一些挑战。掌握以下进阶技巧和常见问题的处理方法至关重要。</p>
<h3>处理动态输入形状 (Dynamic Shapes)</h3>
<ul>
<li><strong>重要性</strong>: 在自然语言处理 (NLP)、自动语音识别 (ASR) 等领域，输入序列的长度（如句子中的单词数、音频的帧数）通常是可变的。模型必须能够处理这种动态性。</li>
<li><strong>方法</strong>: 使用 <code>torch.onnx.export</code> 函数的 <code>dynamic_axes</code> 参数。这个参数是一个字典，键是输入/输出节点的名称，值是另一个字典，指定了哪些轴是动态的，并可以为这些动态轴命名。
                    <pre><code>dynamic_axes = {
    'input_ids': {0: 'batch_size', 1: 'sequence_length'}, # input_ids的第0轴叫batch_size, 第1轴叫sequence_length
    'attention_mask': {0: 'batch_size', 1: 'sequence_length'},
    'logits': {0: 'batch_size'} # 输出logits的第0轴叫batch_size
}
torch.onnx.export(model, dummy_inputs, "model.onnx", ..., dynamic_axes=dynamic_axes)
                    </code></pre>
</li>
<li><strong>挑战</strong>:
                    <ul>
<li>某些 PyTorch 操作（如 <code>view()</code>, <code>reshape()</code>, <code>einops.rearrange</code>）在存在动态维度时，ONNX 导出器可能难以正确推断输出形状，导致转换失败或运行时错误。</li>
<li>像 <code>torch.nn.MultiheadAttention</code> 这样的复杂模块，在处理动态序列长度和 KV Cache 时，其 ONNX 导出尤其具有挑战性。GitHub 上有相关讨论指出特定 PyTorch 版本或导出器（如 TorchDynamo-based）可能对这类问题有更好的处理。例如，一个 PyTorch GitHub issue (#120075) 讨论了 <code>MultiheadAttention</code> 在动态形状导出时遇到的问题，并建议尝试使用 <code>dynamo=True</code> 导出器或更新 PyTorch 版本。 <a href="https://github.com/pytorch/pytorch/issues/120075" target="_blank">PyTorch Issue #120075: MultiheadAttention dynamic shapes</a></li>
</ul>
</li>
</ul>
<h3>处理控制流 (Control Flow)</h3>
<ul>
<li><strong>挑战</strong>: Python 的原生控制流语句（如 <code>if/else</code> 条件判断、<code>for/while</code> 循环）如果其条件或迭代次数依赖于模型中间张量的值（即数据依赖的控制流），传统的基于追踪的 ONNX 导出器 (Trace-based TorchScript) 很难捕捉这种动态行为。追踪只会记录示例输入执行的那一条路径。</li>
<li><strong>解决方案</strong>:
                    <ul>
<li><strong>模型重构</strong>: 最直接的方法是修改模型代码，将数据依赖的控制流替换为 ONNX 可以静态表示或通过特定算子支持的操作。例如，使用掩码 (masking) 和张量操作来模拟条件逻辑。</li>
<li><strong><code>torch.cond()</code></strong> (PyTorch 2.x+): 对于条件分支，PyTorch 提供了 <code>torch.cond()</code> 运算符，它可以在 FX Graph 中表示条件逻辑，并且可以被 TorchDynamo-based 导出器转换为 ONNX 的 <code>If</code> 算子。这要求将原始的 <code>if/else</code> 逻辑封装到两个独立的函数（分别对应 true 和 false 分支）中，并作为参数传递给 <code>torch.cond()</code>。 <a href="https://docs.pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html" target="_blank">PyTorch教程：导出带控制流的模型到ONNX</a></li>
<li><strong>TorchDynamo-based exporter (<code>dynamo=True</code>)</strong>: PyTorch 2.x 引入的基于 TorchDynamo 的导出器对 Python 原生控制流具有更好的支持，因为它通过字节码分析来捕获图，能更准确地理解和转换一些动态行为。然而，它仍然有其局限性，并非所有 Python 控制流都能完美转换。 <a href="https://pytorch.org/docs/stable/onnx_dynamo.html" target="_blank">TorchDynamo-based ONNX Exporter</a></li>
</ul>
</li>
</ul>
<h3>处理不支持的算子 (Unsupported Operators)</h3>
<ul>
<li><strong>现象</strong>: 在调用 <code>torch.onnx.export()</code> 时，可能会遇到错误，提示某个 PyTorch 算子 (operator) 没有对应的 ONNX 实现，或者当前选择的 <code>opset_version</code> 不支持该算子。</li>
<li><strong>诊断</strong>:
                    <ul>
<li>仔细阅读错误信息，它通常会指明是哪个算子出了问题。</li>
<li>使用 Netron 等工具打开（可能部分成功的）导出的 ONNX 文件，检查图结构，定位到出错或行为异常的节点。</li>
</ul>
</li>
<li><strong>解决方案</strong>:
                    <ul>
<li><strong>升级 <code>opset_version</code></strong>: 较新的 ONNX 算子集版本通常会支持更多的 PyTorch 算子。尝试递增 <code>opset_version</code> (e.g., 从 11 到 13, 13 到 17) 看是否能解决问题。但要注意目标推理引擎对高版本 opset 的支持情况。</li>
<li><strong>模型修改</strong>: 如果某个算子确实不被支持或行为不兼容，需要修改 PyTorch 模型代码，用一个或一组功能等效且可导出到 ONNX 的算子来替换它。</li>
<li><strong>自定义算子 (Advanced)</strong>: 对于非常特殊或无法替代的算子，可以为其实现 ONNX 自定义操作。这需要在 PyTorch 端注册导出函数，并在 ONNX Runtime 端提供该自定义算子的 C++ 或 Python 实现。这会增加模型的部署复杂性，并降低其在标准 ONNX 环境中的可移植性。</li>
<li><strong>使用 <code>torch.fx</code> 进行图改写 (Advanced)</strong>: 在模型导出到 ONNX 之前，可以使用 PyTorch FX (<code>torch.fx</code>) 对模型的计算图进行编程方式的修改，例如替换或分解不支持的算子模式。</li>
</ul>
</li>
</ul>
<h3>模型量化 (Quantization) 与导出</h3>
<ul>
<li><strong>目标</strong>: 通过降低模型参数和/或激活值的数值精度（通常从 FP32 转换为 INT8），来显著减小模型体积、降低内存占用，并利用硬件对低精度运算的加速支持来提升推理速度。</li>
<li><strong>PyTorch 中的量化</strong>: PyTorch 提供了多种量化方案 <a href="https://pytorch.org/docs/stable/quantization.html" target="_blank">PyTorch Quantization Documentation</a>:
                    <ul>
<li><strong>训练后量化 (PTQ - Post-Training Quantization)</strong>:
                            <ul>
<li><strong>静态量化 (Static PTQ)</strong>: 对权重和激活值都进行量化。需要在少量有代表性的校准数据上运行模型，以收集激活值的范围信息，从而确定合适的量化参数（scale 和 zero-point）。适用于激活值范围相对稳定的模型（如CNN）。</li>
<li><strong>动态量化 (Dynamic PTQ)</strong>: 仅对权重进行预量化。激活值在推理时动态地被量化和反量化。开销比静态量化大，但不需要校准数据。适用于权重加载是瓶颈的模型（如RNN, Transformer）。</li>
</ul>
</li>
<li><strong>量化感知训练 (QAT - Quantization-Aware Training)</strong>: 在模型训练过程中就模拟量化效应（插入伪量化节点 FakeQuantize）。通常能达到比 PTQ 更好的精度，但训练成本更高。</li>
</ul>
</li>
<li><strong>导出量化模型到 ONNX</strong>:
                    <ul>
<li><code>sherpa-onnx</code> 的 <code>export-onnx.py</code> 脚本示例中，生成的量化模型如 <code><model_name>-encoder.int8.onnx</model_name></code>，这通常是在导出 FP32 ONNX 模型之后，再利用 ONNX Runtime 提供的量化工具（如 <code>onnxruntime.quantization</code> Python API）进行转换得到的。 <a href="https://k2-fsa.github.io/sherpa/onnx/pretrained_models/whisper/export-onnx.html" target="_blank">sherpa-onnx Whisper INT8 models</a></li>
<li>PyTorch 本身也支持将通过其量化API（如 Eager Mode Quantization 或 FX Graph Mode Quantization）处理过的模型导出到 ONNX。这可能需要特定的导出流程，例如先调用 <code>torch.quantization.convert</code>，然后再调用 <code>torch.onnx.export</code>。</li>
<li>ONNX Runtime 的量化工具可以直接作用于 FP32 的 ONNX 模型文件，生成 INT8 的 ONNX 模型。它支持多种量化方法，包括基于校准数据的静态量化。 <a href="https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html" target="_blank">ONNX Runtime Quantization</a></li>
</ul>
</li>
<li><strong>注意事项</strong>:
                    <ul>
<li>量化可能会导致一定程度的精度损失，必须在目标任务上对量化后的模型进行充分评估。</li>
<li>确保目标推理引擎（如特定版本的 ONNX Runtime 或特定硬件的SDK）支持所采用量化方案生成的 ONNX 算子（如 <code>QLinearConv</code>, <code>MatMulInteger</code> 等）。</li>
<li>对于INT8量化，通常需要提供校准数据来获得较好的量化效果。</li>
</ul>
</li>
</ul>
<h3>Transformer 模型 (如 Whisper) 导出特定难点</h3>
<ul>
<li><strong><code>nn.MultiheadAttention</code> 和 KV Cache</strong>:
                    <ul>
<li>KV Cache 的引入是 Transformer 解码器（如 Whisper Decoder）实现高效自回归生成的关键，但它也显著增加了导出到 ONNX 的复杂性。Decoder 的每次前向传播不仅要接收当前的输入 Token 和 Encoder 输出，还要接收上一时刻的 KV Cache 状态，并输出更新后的 KV Cache 状态以及当前 Token 的 Logits。</li>
<li>导出时，必须精确定义所有这些 Cache 张量的名称、数据类型、形状以及它们的动态维度（通常是序列长度维度）。任何不匹配都可能导致导出失败或运行时错误。</li>
<li>Esperanto.ai 的博客中提到，为了更好地适应 ONNX 导出和管理 KV Cache，他们对原始的 Whisper 模型实现进行了一些修改。例如，将一些仅依赖 Encoder 输出的计算（这些计算在原始实现中可能位于 Decoder 内部，并在每个解码步骤中重复执行）移至 Encoder 的末端，从而避免在 Decoder 的自回归循环中重复计算，这是一种有效的优化。 <a href="https://www.esperanto.ai/blog/adapting-whisper-models-to-et-soc-1-architecture-and-exporting-them-to-onnx/" target="_blank">Esperanto.ai: Adapting Whisper for ONNX KV Cache</a></li>
</ul>
</li>
<li><strong>自回归解码逻辑</strong>:
                    <ul>
<li>完整的自回归解码过程（即根据前一个预测的 Token 生成下一个 Token，直到遇到结束符或达到最大长度）通常是在 Python 端（或 C++ 运行时）通过循环调用 ONNX Decoder 模型来实现的。</li>
<li>ONNX 模型本身（如 <code><model_name>-decoder.onnx</model_name></code>）通常只代表单步的 Decoder 计算：给定当前 Tokens、Encoder 输出和过去的 KV Cache，预测下一个 Token 的 Logits 并输出新的 KV Cache。</li>
<li><code>sherpa-onnx</code> 的 <code>export-onnx.py</code> 脚本正是遵循这种设计，将 Decoder 导出为一个单步计算模块。其运行时环境负责管理解码循环和 KV Cache 的传递。</li>
</ul>
</li>
<li><strong>输入音频长度处理</strong>:
                    <ul>
<li>标准的 Whisper 模型通常设计为处理固定长度（例如 30 秒）的音频片段。较短的音频会被填充 (padding)，较长的音频需要被切分 (chunking)。</li>
<li>Esperanto.ai 的博客探讨了一个有趣的优化：通过微调输入音频的长度（例如从 30 秒调整为 28.8 秒），可以使得模型内部张量的维度对特定硬件的内存对齐和向量化操作更友好，从而可能在不显著影响准确率的前提下提升性能。 <a href="https://www.esperanto.ai/blog/adapting-whisper-models-to-et-soc-1-architecture-and-exporting-them-to-onnx/" target="_blank">Esperanto.ai: Audio lengths impact inference speed</a></li>
<li>然而，更通用的做法是通过 <code>dynamic_axes</code> 参数使导出的 ONNX Encoder 模型能够接受可变长度的音频输入（在一定范围内），这样可以减少预处理的复杂性，并更灵活地适应不同时长的音频。</li>
</ul>
</li>
</ul>
<div class="key-points">
<h4>关键要点总结：高级技巧</h4>
<ul>
<li><strong>动态轴 (Dynamic Axes)</strong> 是处理可变序列长度模型的基石。</li>
<li>对于<strong>控制流</strong>，优先考虑模型重构，或使用 <code>torch.cond()</code> 及 TorchDynamo 导出器。</li>
<li>遇到<strong>不支持的算子</strong>，尝试升级 opset，修改模型，或在极端情况下考虑自定义算子。</li>
<li><strong>模型量化</strong>是性能优化的重要手段，需关注精度与硬件支持，ONNX Runtime 提供了强大工具。</li>
<li><strong>Transformer 的 KV Cache</strong> 是导出时的主要复杂点，需精心设计输入输出和动态轴。</li>
</ul>
</div>
</section>
<section id="post-export-validation-usage">
<h2>导出后验证与使用</h2>
<p>成功将 PyTorch 模型导出为 ONNX 格式后，进行彻底的验证是确保模型正确性和可用性的关键步骤。之后，模型便可以在支持 ONNX 的环境中部署使用，例如通过 <code>sherpa-onnx</code>。</p>

<h3>模型可视化与检查</h3>
<ul>
<li><strong>Netron</strong>: 一种用于可视化 ONNX 模型的工具，能够加载 <code>.onnx</code> 文件并展示模型的计算图结构、节点属性、输入输出信息等。 <a href="https://netron.app/" target="_blank">Netron官方网站</a>
<br/>使用 Netron 检查模型时，应关注以下要点：

                    <ul>
<li><strong>输入输出节点</strong>: 名称、形状、数据类型是否与预期一致？<code>dynamic_axes</code> 是否已正确应用？</li>
<li><strong>图结构</strong>: 模型的整体流程是否符合设计？是否存在意外的节点或连接？</li>
<li><strong>算子版本</strong>: 检查关键算子的 <code>domain</code> 和 <code>op_version</code> 是否与目标 ONNX Runtime 环境兼容。</li>
<li><strong>常量和初始值</strong>: 模型的权重和偏置是否已正确嵌入或以外部数据形式关联？</li>
</ul>
</li>
</ul>
<div style="text-align: center; margin: 1.5em 0;">

<p style="font-size: 0.9em; color: #777;">图：Netron 可视化 ONNX 模型示例 (图片来源: Netron GitHub)</p>
</div>
<h3>使用 ONNX Runtime 进行推理验证</h3>
<p>这是验证导出模型数值准确性的核心步骤。目标是确保 ONNX 模型在给定相同输入时，其输出与原始 PyTorch 模型尽可能一致。</p>
<ol>
<li><strong>加载 ONNX 模型</strong>: 使用 ONNX Runtime 的 Python API 创建一个推理会话 (InferenceSession)。
                    <pre><code class="language-python">import onnxruntime as ort
import numpy as np

# 加载ONNX模型
model_path = "your_model.onnx"
session = ort.InferenceSession(model_path, providers=['CPUExecutionProvider']) # 可指定其他EP，如CUDAExecutionProvider

# 获取输入输出名称 (如果导出时未指定，Netron可查看)
input_name = session.get_inputs()[0].name
output_name = session.get_outputs()[0].name
                    </code></pre>
</li>
<li><strong>准备输入数据</strong>:
                    <ul>
<li>准备与 PyTorch 模型推理时使用的完全相同的输入数据。</li>
<li>将输入数据转换为 NumPy 数组，并确保其数据类型 (e.g., <code>np.float32</code>, <code>np.int64</code>) 和形状与 ONNX 模型输入节点的要求一致。</li>
<li>对于有动态轴的输入，确保提供的 NumPy 数组形状在该动态轴的有效范围内。</li>
</ul>
<pre><code class="language-python"># 假设dummy_input是之前用于导出或PyTorch推理的示例输入 (torch.Tensor)
# 需要将其转换为NumPy数组
dummy_input_np = dummy_input.cpu().numpy()
input_feed = {input_name: dummy_input_np}
                    </code></pre>
</li>
<li><strong>执行推理</strong>: 调用会话的 <code>run()</code> 方法。
                    <pre><code class="language-python"># 执行推理
# session.run() 的第一个参数是期望获取的输出节点名称列表
# 第二个参数是以字典形式提供的输入数据，键为输入节点名，值为NumPy数组
onnx_outputs = session.run([output_name], input_feed)
onnx_output_np = onnx_outputs[0] # 通常返回一个列表，对应output_names列表
                    </code></pre>
</li>
<li><strong>与 PyTorch 模型输出对比</strong>:
                    <ul>
<li>使用相同的输入数据，在原始 PyTorch 模型上执行一次推理。</li>
<li>将 PyTorch 模型的输出（通常是 <code>torch.Tensor</code>）转换为 NumPy 数组。</li>
<li>使用 <code>numpy.allclose()</code> 或类似函数比较 ONNX Runtime 的输出和 PyTorch 模型的输出，检查它们在数值上是否足够接近。由于浮点数计算的差异，完全相等可能很难达到，因此需要设置一个合理的容忍度 (<code>atol</code>, <code>rtol</code>)。
                            <pre><code class="language-python"># 获取PyTorch模型输出
# model.eval()
# with torch.no_grad():
#     pytorch_output_tensor = model(dummy_input)
# pytorch_output_np = pytorch_output_tensor.cpu().numpy()

# 比较输出 (假设pytorch_output_np已准备好)
# if np.allclose(pytorch_output_np, onnx_output_np, rtol=1e-03, atol=1e-05):
#     print("ONNX model output matches PyTorch model output within tolerance.")
# else:
#     print("Output mismatch detected!")
#     # 可以进一步打印差异的统计信息
#     # print("Max absolute difference:", np.max(np.abs(pytorch_output_np - onnx_output_np)))
                            </code></pre>
</li>
</ul>
<p>PyTorch 官方的 ONNX 导出教程中也包含了对比 PyTorch 和 ONNX Runtime 结果的步骤。 <a href="https://docs.pytorch.org/tutorials/beginner/onnx/export_simple_model_to_onnx_tutorial.html" target="_blank">PyTorch ONNX Tutorial: Compare Results</a></p>
</li>
</ol>
<h3>在 <code>sherpa-onnx</code> 中使用导出的模型</h3>
<p>一旦 Whisper 模型被成功导出并验证为 ONNX 格式（包括 Encoder, Decoder 和 <code>tokens.txt</code> 文件），它们就可以被 <code>sherpa-onnx</code> 用来进行实际的语音识别任务。</p>
<ul>
<li><code>sherpa-onnx</code> 提供了 C++ 和 Python API，允许开发者加载这些 ONNX 模型文件。</li>
<li>在其内部，<code>sherpa-onnx</code> 的运行时会负责整个语音识别流水线：
                    <ul>
<li><strong>音频预处理</strong>: 包括加载音频、重采样、分帧等。</li>
<li><strong>特征提取</strong>: 计算对数梅尔频谱图，作为 Whisper Encoder 的输入。</li>
<li><strong>Encoder 推理</strong>: 调用 ONNX Encoder 模型处理频谱特征。</li>
<li><strong>Decoder 自回归解码</strong>: 这是一个核心环节。<code>sherpa-onnx</code> 会管理一个解码循环，在循环的每一步：
                            <ul>
<li>将当前已生成的 Tokens、Encoder 的输出以及（如果使用）上一时刻的 KV Cache 状态送入 ONNX Decoder 模型。</li>
<li>获取 Decoder 输出的 Logits，并根据解码策略（如贪心搜索、束搜索 Beam Search）选择下一个 Token。</li>
<li>获取并保存 Decoder 输出的新的 KV Cache 状态，供下一步使用。</li>
</ul>
</li>
<li><strong>后处理</strong>: 将解码得到的 Token ID 序列使用 <code>tokens.txt</code> 文件转换为最终的文本转录。</li>
</ul>
</li>
<li><code>sherpa-onnx</code> 的文档和示例代码会展示如何配置和使用这些导出的 ONNX 模型文件。例如，其 Python API 通常需要指定 Encoder、Decoder 和 Tokens 文件的路径。 <a href="https://k2-fsa.github.io/sherpa/onnx/index.html" target="_blank">sherpa-onnx Documentation</a></li>
</ul>
<p>通过这种方式，<code>sherpa-onnx</code> 充分利用了 ONNX 的跨平台和高性能特性，使得强大的 Whisper 模型能够在各种设备上高效运行。</p>
</section>
<section id="summary-best-practices">
<h2>总结与最佳实践</h2>
<p>将 PyTorch 模型转换为 ONNX 格式是实现模型跨平台部署和性能优化的关键步骤。通过以 <code>sherpa-onnx/scripts/whisper/export-onnx.py</code> 为例的分析，我们深入了解了这一过程的复杂性和关键技术点。</p>
<h3>核心流程回顾</h3>
<ol>
<li><strong>准备 PyTorch 模型</strong>: 确保模型处于评估模式 (<code>model.eval()</code>)，以保证 Dropout、BatchNorm 等层的行为正确。</li>
<li><strong>构造正确的示例输入 (<code>dummy_input</code>)</strong>: 示例输入的形状、数据类型和数量必须与模型实际前向传播时所需的一致。这是 ONNX 导出器追踪计算图的基础。</li>
<li><strong>调用 <code>torch.onnx.export</code></strong>: 这是核心导出函数。需要仔细设置关键参数，特别是：
                    <ul>
<li><code>opset_version</code>: 影响算子支持和兼容性。</li>
<li><code>input_names</code>, <code>output_names</code>: 提高模型的可读性和易用性。</li>
<li><code>dynamic_axes</code>: 对于处理可变长度输入（如文本、音频）至关重要。</li>
</ul>
</li>
<li><strong>组件化导出 (针对复杂模型)</strong>: 对于像 Whisper 这样由多个独立组件（如 Encoder 和 Decoder）构成的复杂模型，通常需要将每个组件分别导出为独立的 ONNX 模型。这更符合实际的推理流程，并有助于管理复杂性。</li>
<li><strong>(可选) 进行 INT8 量化</strong>: 为了进一步优化性能和减小模型体积，可以在导出 FP32 ONNX 模型后，使用 ONNX Runtime 等工具将其转换为 INT8 量化模型。这通常需要校准数据。</li>
<li><strong>验证模型</strong>: 使用 Netron 可视化检查模型结构，并使用 ONNX Runtime 执行推理，将其输出与原始 PyTorch 模型输出进行对比，确保数值准确性。</li>
</ol>
<h3>最佳实践建议</h3>
<ul>
<li><strong>明确 <code>opset_version</code></strong>: 在开始导出前，调研目标推理环境（如特定版本的 ONNX Runtime、硬件SDK）支持的最高 <code>opset_version</code>，并选择一个既能满足模型算子需求又被广泛支持的版本。通常，较新的版本支持更多算子，但兼容性可能稍差。</li>
<li><strong>始终设置 <code>model.eval()</code></strong>: 这是最基本也是最容易被忽略的一点，不正确的模式会导致模型行为错误。</li>
<li><strong>善用 <code>input_names</code>, <code>output_names</code>, <code>dynamic_axes</code></strong>: 清晰地命名输入输出节点，并准确定义动态轴，能极大地方便后续的模型集成和使用，避免因维度不匹配或节点名称混淆导致的问题。</li>
<li><strong>从小处着手，逐步迭代</strong>: 如果模型非常复杂，可以先尝试导出模型的一个小部分或简化版本。成功后再逐步增加复杂性，例如先导出不带 KV Cache 的 Decoder，成功后再尝试导出带 KV Cache 的版本。</li>
<li><strong>充分测试</strong>: 在多种不同的输入数据（包括边界情况、不同序列长度等）下验证导出 ONNX 模型的正确性和性能。确保数值差异在可接受范围内。</li>
<li><strong>查阅官方文档和社区资源</strong>: PyTorch、ONNX、ONNX Runtime 的官方文档是获取最新信息和权威解释的首要来源。GitHub Issues、论坛（如 PyTorch Discuss）和相关项目的社区（如 <code>k2-fsa/sherpa-onnx</code>）也是解决疑难杂症的宝贵资源。</li>
<li><strong>考虑使用 TorchDynamo-based exporter (<code>dynamo=True</code>)</strong>: 对于使用 PyTorch 2.x 及以上版本的用户，特别是处理包含复杂 Python 动态特性或控制流的模型时，新的 TorchDynamo 导出器（通过 <code>torch.onnx.export(..., dynamo=True)</code> 启用）通常能提供更好的支持和更准确的转换结果。 <a href="https://pytorch.org/docs/stable/onnx_dynamo.html" target="_blank">TorchDynamo-based ONNX Exporter</a></li>
<li><strong>版本控制</strong>: 记录用于导出的 PyTorch 版本、ONNX 版本、ONNX Runtime 版本以及 <code>opset_version</code>。这些版本之间的兼容性有时非常敏感。</li>
<li><strong>关注警告信息</strong>: 即使导出成功，也要注意 <code>torch.onnx.export</code> 打印的任何警告信息，它们可能预示着潜在的问题或不兼容性。</li>
</ul>

<section id="references">
<h2>参考资料</h2>
<ul>
<li>PyTorch官方文档：<a href="https://pytorch.org/docs/stable/onnx.html" target="_blank">https://pytorch.org/docs/stable/onnx.html</a></li>
<li>ONNX官方文档：<a href="https://onnx.ai/onnx/intro/" target="_blank">https://onnx.ai/onnx/intro/</a></li>
<li>sherpa-onnx项目：<a href="https://github.com/k2-fsa/sherpa-onnx" target="_blank">https://github.com/k2-fsa/sherpa-onnx</a></li>
<li>ONNX Runtime量化文档：<a href="https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html" target="_blank">https://onnxruntime.ai/docs/performance/model-optimizations/quantization.html</a></li>
<li>Netron可视化工具：<a href="https://netron.app/" target="_blank">https://netron.app/</a></li>
</ul>
</section>

</div>
</body>
</html>
